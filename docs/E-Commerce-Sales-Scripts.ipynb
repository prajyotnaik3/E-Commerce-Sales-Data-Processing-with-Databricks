{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2db5ce93-148c-4873-b46b-fe0d9f1a2255",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date, udf, round, year, sum\n",
    "from pyspark.sql.types import StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d99176a4-48d3-4eb6-97ea-d3361a5b6363",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7286d00-e5dc-48f8-88c8-b688bd6c475e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_absolute_file_path(path):\n",
    "    \"\"\"\n",
    "    Creates absolute path to files in the git repo loaded in the workspace.\n",
    "    Note: Repo name and account have fixed values.\n",
    "    param path: string: file path relative to the root of the git repo\n",
    "    Returns absolute file path on dbfs\n",
    "    \"\"\"\n",
    "    return \"file:/Workspace/Repos/prajyotnaik3pn+2024db1@gmail.com/E-Commerce-Sales-Data-Processing-with-Databricks\" + path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63fe27b8-de54-45b3-afe6-7e90cfde4d72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_delta_table(table, df):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame as a delta table. Creates the table if it doesn't exists. Overwrites the table data (as well as the schema) if table exists.\n",
    "    param table: string: name for delta table\n",
    "    param df: DataFrame: dataframe to be saved as delta table\n",
    "    \"\"\"\n",
    "    df.write.format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec0f1124-590c-4587-ba81-e758c8a869e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_products_raw_df(file_path):\n",
    "    \"\"\"\n",
    "    Reads and creates Spark Dataframe from Product.csv. Renames field names to follow camel case.\n",
    "    param file_path: string: input file path for Product.csv \n",
    "    Returns raw products DataFrame\n",
    "    \"\"\"\n",
    "    cols_mapping = {\"Product ID\": \"productID\", \"Category\": \"category\", \"Sub-Category\": \"subCategory\", \"Product Name\": \"productName\", \"State\": \"state\", \"Price per product\": \"pricePerProduct\"}\n",
    "    return spark.read.format('csv') \\\n",
    "        .option('header', True) \\\n",
    "        .option(\"escape\", '\"') \\\n",
    "        .option(\"quote\", '\"') \\\n",
    "        .load(file_path) \\\n",
    "        .withColumnsRenamed(cols_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e97f9e4b-2c9f-4079-b7a8-7174f51fd80c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_products_enriched_df(table):\n",
    "    \"\"\"\n",
    "    Reads and creates Spark Dataframe from raw delta table. Casts the pricePerProduct field to double.\n",
    "    param table: string: delta table name for raw table to be enriched\n",
    "    Returns enriched products DataFrame\n",
    "    \"\"\"\n",
    "    return spark.table(table) \\\n",
    "        .withColumn(\"pricePerProduct\", col(\"pricePerProduct\").cast(\"double\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9195aef1-3068-49aa-a429-5fe8ce259338",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_customers_raw_df(file_path):\n",
    "    \"\"\"\n",
    "    Reads and creates Spark Dataframe from Customer.xlsx. Renames field names to follow camel case.\n",
    "    param file_path: string: input file path for Customer.xlsx\n",
    "    Returns raw customers DataFrame \n",
    "    \"\"\"\n",
    "    cols_mapping = {\"Customer ID\": \"customerID\", \"Customer Name\": \"customerName\", \"Segment\": \"segment\", \"Country\": \"country\", \"City\": \"city\", \"State\": \"state\", \"Postal Code\": \"postalCode\", \"Region\": \"region\"}\n",
    "    return spark.read.format(\"com.crealytics.spark.excel\") \\\n",
    "        .option(\"header\", True) \\\n",
    "        .option(\"escape\", '\"') \\\n",
    "        .option(\"quote\", '\"') \\\n",
    "        .option(\"sheetName\", \"Worksheet\") \\\n",
    "        .load(file_path) \\\n",
    "        .withColumnsRenamed(cols_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b2d8aa-deb9-4767-8972-727a7c4f0f57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_customers_enriched_df(table):\n",
    "    \"\"\"\n",
    "    Reads and creates Spark Dataframe from raw delta table. \n",
    "    TODO: Cleaning fields\n",
    "    param table: string: delta table name for raw table to be enriched\n",
    "    Returns enriched customers DataFrame\n",
    "    \"\"\"\n",
    "    return spark.table(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ea43488-0428-4ecb-8e1a-8375944f7b9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_orders_raw_df(file_path):\n",
    "    \"\"\"\n",
    "    Reads and creates Spark Dataframe from Order.json. Renames field names to follow camel case.\n",
    "    param file_path: string: input file path for Order.json \n",
    "    Returns raw orders DataFrame\n",
    "    \"\"\"\n",
    "    cols_mapping = {'Row ID': 'rowID', 'Order ID': 'orderID', 'Order Date': 'orderDate', 'Ship Date': 'shipDate', 'Ship Mode': 'shipMode', 'Customer ID': 'customerID', 'Product ID': 'productID', 'Quantity': 'quantity', 'Price': 'price', 'Discount': 'discount', 'Profit':'profit'}\n",
    "    return spark.read.format(\"json\") \\\n",
    "        .option(\"multiline\", True) \\\n",
    "        .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "        .load(file_path) \\\n",
    "        .withColumnsRenamed(cols_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b768a3-050a-48a0-8d92-ab56d6a33fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def clean_price(price):\n",
    "    \"\"\"\n",
    "    Removes a trailing % from price string, if a % exists.\n",
    "    param price: string: price string\n",
    "    Returns cleaned price string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if price.endswith(\"%\"):\n",
    "            price = price[:-1]\n",
    "        return price\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "clean_price_udf = udf(clean_price, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43b1923-5c24-4f47-9142-50a820b705cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_orders_enriched_df(products_table, customers_table, orders_table):\n",
    "    \"\"\"\n",
    "    Reads and creates enriched Spark Dataframe from raw delta tables.\n",
    "    param products_table: string: delta table name fore enriched products table\n",
    "    param customers_table: string: delta table name fore enriched customers table\n",
    "    param table: string: delta table name for raw table to be enriched\n",
    "    Returns enriched orders DataFrame\n",
    "    \"\"\"\n",
    "    productsDFEnriched = spark.table(products_table) \\\n",
    "        .select(col(\"productID\"), col(\"category\"), col(\"subCategory\")) \\\n",
    "        .distinct()\n",
    "    customersDFEnriched = spark.table(customers_table) \\\n",
    "        .select(col(\"customerID\"), col(\"customerName\"), col(\"country\"))\n",
    "    ordersDF = spark.table(orders_table) \\\n",
    "        .withColumn(\"orderDate\", to_date(col(\"orderDate\"), \"d/M/yyyy\")) \\\n",
    "        .withColumn(\"shipDate\", to_date(col(\"shipDate\"), \"d/M/yyyy\")) \\\n",
    "        .withColumn(\"price\", round(clean_price_udf(col(\"price\")).cast(\"double\"),2))\n",
    "    return ordersDF.join(customersDFEnriched, on=[\"customerID\"], how=\"inner\") \\\n",
    "        .join(productsDFEnriched, on=[\"productID\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0adbbeb-1995-456b-80cd-053ff80b719c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_orders_aggregated_df(table):\n",
    "    \"\"\"\n",
    "    Creates aggregated spark DataFrame from enriched table, by grouping year, category, subCategory and customerID\n",
    "    param table: string: enriched orders delta table to be aggregated\n",
    "    Returns aggregated orders DataFrame\n",
    "    \"\"\"\n",
    "    ordersDFEnriched = spark.table(table) \\\n",
    "        .withColumn(\"year\", year(col(\"orderDate\")))\n",
    "    return ordersDFEnriched.groupBy(col(\"year\"), col(\"category\"), col(\"subCategory\"), col(\"customerID\")) \\\n",
    "        .agg(sum(col(\"profit\")).alias(\"profit\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "E-Commerce-Sales-Scripts",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
