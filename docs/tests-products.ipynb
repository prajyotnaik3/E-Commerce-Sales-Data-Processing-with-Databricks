{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ab3fac2-af74-4840-b984-640b5557abe3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "from pyspark.errors.exceptions.captured import AnalysisException\n",
    "from pyspark.sql.types import StructField, StringType, DoubleType, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ae8ac11-ff4b-477f-a3a4-c911833e0403",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../scripts/E-Commerce-Sales-Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18c0db4a-8ee2-4b74-bb7b-51b198b7f113",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_absolute_file_path (__main__.TestProductsIngestion) ... ok\ntest_create_products (__main__.TestProductsIngestion) ... /usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=51, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 34474), raddr=('127.0.0.1', 39265)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=51, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 53388), raddr=('127.0.0.1', 40965)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=51, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50664), raddr=('127.0.0.1', 36019)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=51, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 54972), raddr=('127.0.0.1', 36759)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\nok\ntest_nonexistent_file (__main__.TestProductsIngestion) ... ok\n\n----------------------------------------------------------------------\nRan 3 tests in 12.624s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "class TestProductsIngestion(unittest.TestCase):\n",
    "\n",
    "    def test_absolute_file_path(self):\n",
    "        file_path = \"/data/Product.csv\"\n",
    "        absolute_file_path = get_absolute_file_path(file_path)\n",
    "        expected_absolute_file_path = \"file:/Workspace/Repos/prajyotnaik3pn+2024db1@gmail.com/E-Commerce-Sales-Data-Processing-with-Databricks/data/Product.csv\"\n",
    "        self.assertTrue(absolute_file_path)\n",
    "        self.assertEqual(absolute_file_path, expected_absolute_file_path)\n",
    "    \n",
    "    def test_nonexistent_file(self):\n",
    "        file_path = \"/data/Product.csv\"\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            create_products_raw_df(file_path)\n",
    "\n",
    "    def test_create_products(self):\n",
    "        file_path = get_absolute_file_path(\"/data/Product.csv\")\n",
    "        expected_columns_raw = [('productID', 'string'), ('category', 'string'), ('subCategory', 'string'),  ('productName', 'string'), ('state', 'string'), ('pricePerProduct', 'string')]\n",
    "        table_raw = \"ecommerce_sales_test.products_raw\"\n",
    "        expected_columns_enriched = [('productID', 'string'), ('category', 'string'), ('subCategory', 'string'),  ('productName', 'string'), ('state', 'string'), ('pricePerProduct', 'double')]\n",
    "        table_enriched = \"ecommerce_sales_test.products_enriched\"\n",
    "\n",
    "        df = create_products_raw_df(file_path)\n",
    "\n",
    "        self.assertTrue(df)\n",
    "        self.assertEqual(df.dtypes, expected_columns_raw)\n",
    "        self.assertEqual(df.filter(df[\"productID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"category\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"subCategory\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"productName\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"state\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"pricePerProduct\"].isNull()).count(), 0)\n",
    "\n",
    "        create_delta_table(table_raw, df)\n",
    "\n",
    "        df2 = spark.table(table_raw)\n",
    "\n",
    "        self.assertEqual(df.dtypes, df2.dtypes)\n",
    "        self.assertEqual(df.exceptAll(df2).rdd.isEmpty(), True)\n",
    "        self.assertEqual(df2.exceptAll(df).rdd.isEmpty(), True)\n",
    "\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            df3 = create_products_enriched_df(\"ecommerce_sales_test.products_raw_temp\")\n",
    "\n",
    "        df3 = create_products_enriched_df(table_raw)\n",
    "\n",
    "        self.assertTrue(df3)\n",
    "        self.assertEqual(df3.dtypes, expected_columns_enriched)\n",
    "        self.assertEqual(df3.filter(df3[\"productID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"category\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"subCategory\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"productName\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"state\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"pricePerProduct\"].isNull()).count(), 0)\n",
    "\n",
    "        create_delta_table(table_enriched, df3)\n",
    "        df4 = spark.table(table_enriched)\n",
    "\n",
    "        self.assertEqual(df3.dtypes, df4.dtypes)\n",
    "        self.assertEqual(df3.exceptAll(df4).rdd.isEmpty(), True)\n",
    "        self.assertEqual(df4.exceptAll(df3).rdd.isEmpty(), True)\n",
    "\n",
    "\n",
    "r = unittest.main(argv=[''], verbosity=2, exit=False)\n",
    "assert r.result.wasSuccessful(), 'Test failed; see logs above'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1700342145905719,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "tests-products",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
