{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ec347ef-7e87-4fa3-8780-860f1f21b589",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "from pyspark.errors.exceptions.captured import AnalysisException\n",
    "from pyspark.sql.types import StructField, StringType, DoubleType, StructType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cd1be0b-554d-44f8-b2b8-618b974209d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../scripts/E-Commerce-Sales-Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94db48c-e37d-418c-9857-eb147510d629",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_clean_price (__main__.TestReadJsonFile) ... ok\ntest_create_orders (__main__.TestReadJsonFile) ... /usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 45888), raddr=('127.0.0.1', 35371)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 41152), raddr=('127.0.0.1', 39351)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 38704), raddr=('127.0.0.1', 43323)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 47498), raddr=('127.0.0.1', 43577)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 38872), raddr=('127.0.0.1', 44729)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 40148), raddr=('127.0.0.1', 35135)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/databricks/spark/python/pyspark/sql/pandas/utils.py:37: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(pandas.__version__) < LooseVersion(minimum_pandas_version):\n/databricks/spark/python/pyspark/sql/pandas/utils.py:64: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n  if LooseVersion(pyarrow.__version__) < LooseVersion(minimum_pyarrow_version):\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 57056), raddr=('127.0.0.1', 44195)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 56298), raddr=('127.0.0.1', 44231)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\n/usr/lib/python3.10/socket.py:776: ResourceWarning: unclosed <socket.socket fd=65, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50690), raddr=('127.0.0.1', 41645)>\n  self._sock = None\nResourceWarning: Enable tracemalloc to get the object allocation traceback\nok\ntest_nonexistent_file (__main__.TestReadJsonFile) ... ok\n\n----------------------------------------------------------------------\nRan 3 tests in 29.148s\n\nOK\n"
     ]
    }
   ],
   "source": [
    "class TestReadJsonFile(unittest.TestCase):\n",
    "    \n",
    "    def test_nonexistent_file(self):\n",
    "        file_path = \"/data/Order.json\"\n",
    "        with self.assertRaises(AnalysisException):\n",
    "            create_products_raw_df(file_path)\n",
    "    \n",
    "    def test_clean_price(self):\n",
    "        self.assertEqual(clean_price(\"100\"), \"100\")\n",
    "        self.assertEqual(clean_price(\"100%\"), \"100\")\n",
    "\n",
    "    def test_create_orders(self):\n",
    "        file_path = get_absolute_file_path(\"/data/Order.json\")\n",
    "        expected_columns_raw = [('rowID', 'bigint'), ('orderID', 'string'), ('orderDate', 'string'),  ('shipDate', 'string'), ('shipMode', 'string'), ('customerID', 'string'), ('productID', 'string'), ('quantity', 'bigint'), ('price', 'string'), ('discount', 'double'), ('profit', 'double')]\n",
    "        table_raw = \"ecommerce_sales_test.orders_raw\"\n",
    "        expected_columns_enriched = [('rowID', 'bigint'), ('orderID', 'string'), ('orderDate', 'date'),  ('shipDate', 'date'), ('shipMode', 'string'), ('customerID', 'string'), ('productID', 'string'), ('quantity', 'bigint'), ('price', 'double'), ('discount', 'double'), ('profit', 'double'), ('category', 'string'), ('subCategory', 'string'), ('customerName', 'string'), ('country', 'string')]\n",
    "        table_enriched = \"ecommerce_sales_test.orders_enriched\"\n",
    "        expected_columns_aggregated = [('customerID', 'string'), ('profit', 'double'), ('category', 'string'), ('subCategory', 'string'), ('year', 'int')]\n",
    "        table_aggregated = \"ecommerce_sales_test.orders_aggregated\"\n",
    "\n",
    "        df = create_orders_raw_df(file_path)\n",
    "\n",
    "        self.assertTrue(df)\n",
    "        df_dtypes = {dtype[0]: dtype[1] for dtype in df.dtypes}\n",
    "        expected_columns_raw_dtypes = {dtype[0]: dtype[1] for dtype in expected_columns_raw}\n",
    "        self.assertEqual(df_dtypes, expected_columns_raw_dtypes)\n",
    "        self.assertEqual(df.filter(df[\"rowID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"orderID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"orderDate\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"shipDate\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"shipMode\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"customerID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"productID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"quantity\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"price\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"discount\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df.filter(df[\"profit\"].isNull()).count(), 0)\n",
    "        \n",
    "        create_delta_table(table_raw, df)\n",
    "        df2 = spark.table(table_raw)\n",
    "\n",
    "        self.assertTrue(df2)\n",
    "        df_dtypes = {dtype[0]: dtype[1] for dtype in df.dtypes}\n",
    "        df2_dtypes = {dtype[0]: dtype[1] for dtype in df2.dtypes}\n",
    "        self.assertEqual(df_dtypes, df2_dtypes)\n",
    "        self.assertEqual(df.exceptAll(df2).rdd.isEmpty(), True)\n",
    "        self.assertEqual(df2.exceptAll(df).rdd.isEmpty(), True)\n",
    "        \n",
    "        with self.assertRaises(AnalysisException):\n",
    "            df3 = create_products_enriched_df(\"ecommerce_sales_test.orders_raw_temp\")\n",
    "        \n",
    "        df3 = create_orders_enriched_df(\"ecommerce_sales_test.products_raw\", \"ecommerce_sales_test.customers_raw\", table_raw)\n",
    "\n",
    "        self.assertTrue(df3)\n",
    "        df3_dtypes = {dtype[0]: dtype[1] for dtype in df3.dtypes}\n",
    "        expected_columns_enriched_dtypes = {dtype[0]: dtype[1] for dtype in expected_columns_enriched}\n",
    "        self.assertEqual(df3_dtypes, expected_columns_enriched_dtypes)\n",
    "        self.assertEqual(df3.filter(df3[\"rowID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"orderID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"orderDate\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"shipDate\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"shipMode\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"customerID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"productID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"quantity\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"price\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"discount\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"profit\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df3.filter(df3[\"category\"].isNull()).count(), 204)\n",
    "        self.assertEqual(df3.filter(df3[\"subCategory\"].isNull()).count(), 204)\n",
    "        self.assertEqual(df3.filter(df3[\"customerName\"].isNull()).count(), 137)\n",
    "        self.assertEqual(df3.filter(df3[\"country\"].isNull()).count(), 0)\n",
    "        \n",
    "        create_delta_table(table_enriched, df3)\n",
    "        df4 = spark.table(table_enriched)\n",
    "        self.assertEqual(df3.dtypes, df4.dtypes)\n",
    "        self.assertEqual(df3.exceptAll(df4).rdd.isEmpty(), True)\n",
    "        self.assertEqual(df4.exceptAll(df3).rdd.isEmpty(), True)\n",
    "        \n",
    "        with self.assertRaises(AnalysisException):\n",
    "            df5 = create_products_enriched_df(\"ecommerce_sales_test.orders_enriched_temp\")\n",
    "        \n",
    "        df5 = create_orders_aggregated_df(table_enriched)\n",
    "        \n",
    "        self.assertTrue(df5)\n",
    "        df5_dtypes = {dtype[0]: dtype[1] for dtype in df5.dtypes}\n",
    "        expected_columns_aggregated_dtypes = {dtype[0]: dtype[1] for dtype in expected_columns_aggregated}\n",
    "        self.assertEqual(df5_dtypes, expected_columns_aggregated_dtypes)\n",
    "        self.assertEqual(df5.filter(df5[\"customerID\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df5.filter(df5[\"profit\"].isNull()).count(), 0)\n",
    "        self.assertEqual(df5.filter(df5[\"category\"].isNull()).count(), 192)\n",
    "        self.assertEqual(df5.filter(df5[\"subCategory\"].isNull()).count(), 192)\n",
    "        self.assertEqual(df5.filter(df5[\"year\"].isNull()).count(), 0)\n",
    "\n",
    "        self.assertTrue(set(df5.select(col(\"year\")).distinct().toPandas().year).difference({2014, 2015, 2016, 2017}) == set())\n",
    "\n",
    "        create_delta_table(table_aggregated, df5)\n",
    "        df6 = spark.table(table_aggregated)\n",
    "        self.assertEqual(df5.dtypes, df6.dtypes)\n",
    "        self.assertEqual(df5.exceptAll(df6).rdd.isEmpty(), True)\n",
    "        self.assertEqual(df6.exceptAll(df5).rdd.isEmpty(), True)\n",
    "\n",
    "r = unittest.main(argv=[''], verbosity=2, exit=False)\n",
    "assert r.result.wasSuccessful(), 'Test failed; see logs above'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "tests-orders",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
